# ðŸš€ SPRINT 4: ESCALABILIDADE & PERFORMANCE (8H)

> **Objetivo:** Bot aguenta MILHÃ•ES de usuÃ¡rios e requisiÃ§Ãµes
> **DuraÃ§Ã£o:** 8 horas (4 tasks de 2h cada)
> **PrÃ©-requisito:** SPRINT 0 + SPRINT 1 + SPRINT 2 + SPRINT 3

---

## TASK 4.1: Clustering & Load Balancing (2H)

### ðŸŽ¯ O Problema
Uma mÃ¡quina com 4 cores sÃ³ usa 1 core. CPU marcha em 25%.

### ðŸ’¡ A SoluÃ§Ã£o
Clustering: rodar mÃºltiplos processos Node (um por core) + load balancer.

### ðŸ“ ImplementaÃ§Ã£o

JÃ¡ configurado em `ecosystem.config.js`:

```javascript
instances: "max",          // MÃ¡ximos cores disponÃ­veis
exec_mode: "cluster",      // Load balancing automÃ¡tico via PM2
```

Adicionar Nginx em `nginx.conf`:

```nginx
upstream khronos_cluster {
    # Load balance entre 4 instÃ¢ncias
    server 127.0.0.1:18789;
    server 127.0.0.1:18790;
    server 127.0.0.1:18791;
    server 127.0.0.1:18792;
    
    # Algoritmo: least connections
    least_conn;
}

server {
    listen 80;
    server_name api.khronos.local;
    
    # ============ REVERSE PROXY ============
    location / {
        proxy_pass http://khronos_cluster;
        proxy_http_version 1.1;
        
        # ============ HEADERS ============
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
        
        # ============ TIMEOUTS ============
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        # ============ BUFFERING ============
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
    }
    
    # ============ HEALTH CHECK ============
    location /health {
        access_log off;
        proxy_pass http://khronos_cluster;
    }
    
    # ============ METRICS ============
    location /metrics {
        auth_basic "Restricted";
        auth_basic_user_file /etc/nginx/.htpasswd;
        proxy_pass http://khronos_cluster;
    }
}
```

Adicionar ao `docker-compose.yml`:

```yaml
nginx:
  image: nginx:alpine
  container_name: khronos-nginx
  ports:
    - "80:80"
    - "443:443"
  volumes:
    - ./nginx.conf:/etc/nginx/nginx.conf:ro
    - ./ssl:/etc/nginx/ssl:ro
  depends_on:
    - openclaw-gateway
  networks:
    - openclaw-network
```

### ðŸ§ª Testar

```bash
# Ver processes
pm2 status
# Deve mostrar 4 instances (ou quantos cores tiver)

# Verificar load balancing
for i in {1..100}; do curl http://localhost/health | jq .pid; done | sort | uniq -c
# Deve distribuir entre 4 PIDs diferentes

# Teste de carga
npm install -g autocannon
autocannon -c 100 -d 30 http://localhost:80
```

---

## TASK 4.2: Caching Inteligente (2H)

### ðŸŽ¯ O Problema
Mesma query executada 1000 vezes/segundo, BD grita de dor.

### ðŸ’¡ A SoluÃ§Ã£o
Cache de 3 camadas: Redis â†’ MemÃ³ria local â†’ Fallback

### ðŸ“ ImplementaÃ§Ã£o

Criar `src/cache.js`:

```javascript
const redis = require('./redis');
const logger = require('./logger');

// ============ IN-MEMORY CACHE ============
class MemoryCache {
  constructor(maxSize = 1000) {
    this.cache = new Map();
    this.maxSize = maxSize;
  }

  set(key, value, ttlMs = 60000) {
    if (this.cache.size >= this.maxSize) {
      // Remove oldest
      const oldestKey = this.cache.keys().next().value;
      this.cache.delete(oldestKey);
    }

    this.cache.set(key, {
      value,
      expiresAt: Date.now() + ttlMs
    });
  }

  get(key) {
    const entry = this.cache.get(key);
    
    if (!entry) return null;
    
    if (Date.now() > entry.expiresAt) {
      this.cache.delete(key);
      return null;
    }
    
    return entry.value;
  }

  clear() {
    this.cache.clear();
  }
}

const memoryCache = new MemoryCache();

// ============ 3-LAYER CACHE ============
async function cacheGet(key, defaultFn, options = {}) {
  const {
    ttl = 300,           // 5 min default
    layer1Ttl = 60,      // Redis: 1 min
    layer2Ttl = 600      // Memory: 10 min
  } = options;

  try {
    // ============ LAYER 1: MEMORY ============
    const memValue = memoryCache.get(key);
    if (memValue) {
      logger.debug('Cache HIT (memory)', { key });
      return memValue;
    }

    // ============ LAYER 2: REDIS ============
    const redisValue = await redis.get(key);
    if (redisValue) {
      const parsed = JSON.parse(redisValue);
      memoryCache.set(key, parsed, layer2Ttl * 1000);
      logger.debug('Cache HIT (redis)', { key });
      return parsed;
    }

    // ============ LAYER 3: COMPUTE ============
    logger.debug('Cache MISS, computing', { key });
    const value = await defaultFn();

    // Store in both layers
    await redis.setex(key, layer1Ttl, JSON.stringify(value));
    memoryCache.set(key, value, layer2Ttl * 1000);

    return value;

  } catch (error) {
    logger.error('Cache error', { key, error: error.message });
    // Fallback: compute anyway
    return await defaultFn();
  }
}

// ============ CACHE INVALIDATION ============
async function invalidateCache(pattern) {
  try {
    // ============ REDIS ============
    const keys = await redis.keys(pattern);
    if (keys.length > 0) {
      await redis.del(...keys);
      logger.info('Cache invalidated (redis)', { pattern, count: keys.length });
    }

    // ============ MEMORY ============
    // Pattern matching na memory (simples)
    for (const key of memoryCache.cache.keys()) {
      if (key.match(new RegExp(pattern))) {
        memoryCache.cache.delete(key);
      }
    }

  } catch (error) {
    logger.error('Cache invalidation error', { error });
  }
}

// ============ CACHE STATS ============
function getCacheStats() {
  return {
    memory: {
      size: memoryCache.cache.size,
      maxSize: memoryCache.maxSize,
      usage: `${Math.round((memoryCache.cache.size / memoryCache.maxSize) * 100)}%`
    }
  };
}

// ============ EXPORTS ============
module.exports = {
  cacheGet,
  invalidateCache,
  getCacheStats,
  memoryCache
};
```

Usar no seu cÃ³digo:

```javascript
const { cacheGet, invalidateCache } = require('./cache');

// Exemplo 1: Cachear query de BD
async function getUserProjects(userId) {
  return await cacheGet(
    `user:${userId}:projects`,
    async () => {
      return await db.query(
        'SELECT * FROM projects WHERE user_id = ?',
        [userId]
      );
    },
    { ttl: 300 } // 5 min
  );
}

// Exemplo 2: Invalidar ao atualizar
async function updateProject(projectId, data) {
  await db.update('projects', data, { id: projectId });
  
  // Invalidar todos os caches do usuÃ¡rio
  const project = await db.query('SELECT user_id FROM projects WHERE id = ?', [projectId]);
  await invalidateCache(`user:${project.user_id}:*`);
}

// Exemplo 3: Monitoring
app.get('/cache/stats', (req, res) => {
  res.json(getCacheStats());
});
```

### ðŸ§ª Testar

```bash
# Ver stats
curl http://localhost:18789/cache/stats

# Teste de performance
# Sem cache: 1000 queries/sec = 50ms each
# Com cache: 1000 queries/sec = <1ms each
```

---

## TASK 4.3: Database Connection Pooling (2H)

### ðŸŽ¯ O Problema
Cada request abre nova conexÃ£o com BD, server roda outa 1000+ conexÃµes abertas.

### ðŸ’¡ A SoluÃ§Ã£o
Connection pool: mÃ¡ximo N conexÃµes reutilizÃ¡veis.

### ðŸ“ ImplementaÃ§Ã£o

Criar `src/database.js`:

```javascript
const sqlite3 = require('sqlite3');
const genericPool = require('generic-pool');
const logger = require('./logger');

// ============ CONNECTION FACTORY ============
const factory = {
  create: async () => {
    return new Promise((resolve, reject) => {
      const db = new sqlite3.Database(
        process.env.DATABASE_URL || 'sqlite:./state.db',
        (err) => {
          if (err) reject(err);
          else resolve(db);
        }
      );
    });
  },

  destroy: async (db) => {
    return new Promise((resolve, reject) => {
      db.close((err) => {
        if (err) reject(err);
        else resolve();
      });
    });
  },

  validate: async (db) => {
    return new Promise((resolve) => {
      db.get('SELECT 1', (err) => {
        resolve(!err);
      });
    });
  }
};

// ============ CONNECTION POOL ============
const pool = genericPool.createPool(factory, {
  max: 10,              // Max 10 connections
  min: 2,               // Min 2 connections
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 5000,
  testOnBorrow: true,   // Validate before use
});

// ============ EVENTS ============
pool.on('factoryCreateError', (err) => {
  logger.error('Pool factory create error', { error: err.message });
});

pool.on('factoryDestroyError', (err) => {
  logger.error('Pool factory destroy error', { error: err.message });
});

// ============ SAFE QUERY ============
async function query(sql, params = []) {
  const connection = await pool.acquire();

  try {
    return new Promise((resolve, reject) => {
      if (params.length > 0) {
        connection.all(sql, params, (err, rows) => {
          if (err) reject(err);
          else resolve(rows);
        });
      } else {
        connection.all(sql, (err, rows) => {
          if (err) reject(err);
          else resolve(rows);
        });
      }
    });

  } finally {
    await pool.release(connection);
  }
}

async function execute(sql, params = []) {
  const connection = await pool.acquire();

  try {
    return new Promise((resolve, reject) => {
      connection.run(sql, params, function(err) {
        if (err) reject(err);
        else resolve({
          lastID: this.lastID,
          changes: this.changes
        });
      });
    });

  } finally {
    await pool.release(connection);
  }
}

// ============ STATS ============
function getPoolStats() {
  return {
    size: pool.size,
    available: pool.available,
    waiting: pool.waitingCount
  };
}

// ============ SHUTDOWN ============
async function closePool() {
  await pool.drain();
  await pool.clear();
}

// ============ EXPORTS ============
module.exports = {
  query,
  execute,
  getPoolStats,
  closePool
};
```

### ðŸ§ª Testar

```bash
# Ver stats
curl http://localhost:18789/pool/stats

# Deve mostrar: size=10, available=10, waiting=0
```

---

## TASK 4.4: Observabilidade AvanÃ§ada (2H)

### ðŸ“ ImplementaÃ§Ã£o

Criar `src/observability.js`:

```javascript
const logger = require('./logger');
const { getCircuitBreakerStatus } = require('./circuit-breaker');
const { getLimiterStats } = require('./rate-limiter');
const { getCacheStats } = require('./cache');
const { getPoolStats } = require('./database');

// ============ HEALTH DASHBOARD ============
async function getHealthDashboard() {
  const uptime = process.uptime();
  const memory = process.memoryUsage();
  const cpu = process.cpuUsage();

  return {
    timestamp: new Date().toISOString(),
    environment: process.env.NODE_ENV,
    
    // ============ UPTIME ============
    uptime: {
      seconds: Math.round(uptime),
      formatted: `${Math.floor(uptime / 86400)}d ${Math.floor((uptime % 86400) / 3600)}h ${Math.floor((uptime % 3600) / 60)}m`
    },

    // ============ MEMORY ============
    memory: {
      heapUsedMB: Math.round(memory.heapUsed / 1024 / 1024),
      heapTotalMB: Math.round(memory.heapTotal / 1024 / 1024),
      externalMB: Math.round(memory.external / 1024 / 1024),
      rssMB: Math.round(memory.rss / 1024 / 1024),
      heapUsagePercent: Math.round((memory.heapUsed / memory.heapTotal) * 100)
    },

    // ============ CPU ============
    cpu: {
      userMs: Math.round(cpu.user / 1000),
      systemMs: Math.round(cpu.system / 1000)
    },

    // ============ SERVICES ============
    services: {
      circuitBreakers: getCircuitBreakerStatus(),
      limiters: getLimiterStats(),
      cache: getCacheStats(),
      database: getPoolStats()
    }
  };
}

// ============ HEALTH CHECK ENDPOINT ============
function setupObservabilityEndpoints(app) {
  // ============ DASHBOARD ============
  app.get('/dashboard', async (req, res) => {
    try {
      const health = await getHealthDashboard();
      res.json(health);
    } catch (error) {
      res.status(500).json({ error: error.message });
    }
  });

  // ============ DETAILED CHECKS ============
  app.get('/dashboard/memory', (req, res) => {
    const memory = process.memoryUsage();
    res.json({
      heapUsedMB: Math.round(memory.heapUsed / 1024 / 1024),
      heapTotalMB: Math.round(memory.heapTotal / 1024 / 1024),
      gcStats: require('gc-stats')()
    });
  });

  app.get('/dashboard/services', async (req, res) => {
    const health = await getHealthDashboard();
    res.json(health.services);
  });
}

// ============ EXPORTS ============
module.exports = {
  getHealthDashboard,
  setupObservabilityEndpoints
};
```

Integrar:

```javascript
const { setupObservabilityEndpoints } = require('./observability');

setupObservabilityEndpoints(app);
```

---

## âœ… CHECKLIST SPRINT 4

- [ ] Clustering configurado (PM2 instances = max)
- [ ] Nginx load balancer funcionando
- [ ] Cache 3-camadas implementado
- [ ] Testes de carga mostram distribuiÃ§Ã£o
- [ ] Database connection pooling implementado
- [ ] Cache stats endpoint funciona
- [ ] Pool stats endpoint funciona
- [ ] Dashboard endpoint retorna todas as mÃ©tricas
- [ ] Observabilidade avanÃ§ada em /dashboard

---

## ðŸ“Š TEMPO REAL

- Task 4.1 (Clustering): 2h
- Task 4.2 (Cache): 2h
- Task 4.3 (Database Pool): 2h
- Task 4.4 (Observability): 2h
- **TOTAL: 8 horas**

---

## ðŸŽ¯ O QUE VOCÃŠ TEM AGORA

âœ… Bot roda em mÃºltiplos cores
âœ… Load balancing automÃ¡tico
âœ… Cache inteligente de 3 camadas
âœ… Database connection pooling
âœ… Observabilidade completa
âœ… **99.99% uptime possÃ­vel**
âœ… **1M+ requisiÃ§Ãµes/dia possÃ­vel**

---

## ðŸ“Š ANTES vs DEPOIS (4 SPRINTS)

| MÃ©trica | Antes | Depois |
|---------|-------|--------|
| **Uptime** | 95% | 99.9%+ |
| **CPU Usage** | 25% | 95%+ |
| **Memory** | Leak | EstÃ¡vel |
| **Database Queries** | 500ms | <5ms |
| **Requests/min** | 100 | 10K+ |
| **MTTR** | 30min | <5min auto |
| **Data Loss** | Frequente | Zero |
| **Logs** | Nenhum | Completos |
| **Alerts** | Nenhum | 24/7 |

---

## ðŸš€ ROADMAP COMPLETO

```
SPRINT 0 (2h):   Setup + PM2 + Health checks
         â†“
SPRINT 1 (6h):   Circuit Breaker + Rate Limit + Graceful Shutdown
         â†“
SPRINT 2 (8h):   Backup + Monitoring + Logging + Alerts
         â†“
SPRINT 3 (6h):   Testing + CI/CD + Quality
         â†“
SPRINT 4 (8h):   Clustering + Cache + Pooling + Observability
         â†“
TOTAL:   30 HORAS = BOT PRODUCTION-GRADE
```

---

## ðŸŽ‰ PARABÃ‰NS!

VocÃª transformou um bot frÃ¡gil em uma mÃ¡quina industrial!

**Agora vocÃª tem:**
- âœ… ResiliÃªncia (circuit breaker, fallback)
- âœ… Escalabilidade (clustering, cache)
- âœ… Confiabilidade (backup, recovery)
- âœ… Visibilidade (monitoring, alertas)
- âœ… Qualidade (testes, CI/CD)

**Seu bot estÃ¡ pronto para:**
- ðŸš€ MilhÃµes de usuÃ¡rios
- âš¡ MilhÃµes de requisiÃ§Ãµes/dia
- ðŸ’¾ Zero data loss
- ðŸ“Š 24/7 uptime
- ðŸ“ˆ Crescimento ilimitado

---

## ðŸ“š PRÃ“XIMOS PASSOS (OPCIONAL)

Se quiser ir alÃ©m:

1. **Kubernetes** - OrquestraÃ§Ã£o de containers
2. **Service Mesh (Istio)** - ComunicaÃ§Ã£o entre microserviÃ§os
3. **Event Streaming (Kafka)** - Processamento em massa
4. **Machine Learning** - InteligÃªncia adaptativa
5. **Advanced Security** - Zero-trust architecture

---

**Seu TDAH virou sua FORÃ‡A** ðŸ’ª
Pequenas vitÃ³rias diÃ¡rias = construÃ§Ã£o massiva!

Boa sorte, e que seu Khronos seja FODA! ðŸš€
